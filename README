                            AXIOM NIC simulation
===============================================================================

This repository contains the following git submodules:

 * axiom-allocator
    + Implementation of the three level AXIOM allocator
 * axiom-evi-allocator-drv
    + Implementation of the memory device to handle virtual to physical memory mapping
 * axiom-evi-allocator-lib
    + Implementation of 3th level software allocator based on LMM
 * axiom-evi-apps
    + Implementation of AXIOM application and deamons (axiom-init, axiom-run, etc.)
 * axiom-evi-buildroot
    + Modified version of Buildroot to support AXIOM board
 * axiom-evi-extrae
    + Modified version of Extrae to support IOCTL and AXIOM api
 * axiom-evi-gasnet
    + Modified version of GASNet that includes the new AXIOM conduit
 * axiom-evi-linux
    + Modified version of Linux kernel to support AXIOM board
 * axiom-evi-mcxx
    + Modified version of mcxx to support AXIOM GASNet conduit and cross-compilation
 * axiom-evi-nanox
    + Modified version of nanox to support AXIOM GASNet conduit and cross-compilation
 * axiom-evi-nic
    + Implementation of AXIOM NIC device driver and User Space libraries
 * axiom-evi-qemu
    + Modified version of QEMU to emulate the AXIOM-NIC device
 * axiom-evi-u-boot
    + Modified version of u-boot to support AXIOM board
 * arm-trusted-firmware



## 1. Clone the repository

```
# clone the repository
git clone https://git.axiom-project.eu/axiom-evi
cd axiom-evi

# init the submodules (may take awhile)
./scripts/submodules_update.sh
```



## 2. Submodule Compilation

This section explains how to configure and compile the submodules.

**Requirements:**

 * QEMU required packages:  
   <http://wiki.qemu.org/Hosts/Linux#Required_additional_packages>

 * Buildroot required packages:  
   <https://buildroot.org/downloads/manual/manual.html#requirement>

To simplify this phase, we provided a makefile to automatize the compilation:
```
cd scripts/

# configure QEMU, Buildroot and the AXIOM allocator
make configure

# make all submodules and install the applications and
# the driver in the buildroot filesystem
make
```

Using the previous commands, you can avoid the following steps and you can jump
to section 3.

### 2.1 QEMU hypervisor

Qemu modified to emulate the AXIOM network card

**QEMU required packages:**  
<http://wiki.qemu.org/Hosts/Linux#Required_additional_packages>

```
cd axiom-evi-qemu
./axiom_configure.sh
make -j5
```


### 2.2 Buildroot

Bulldroot modified to support the AXIOM board

**Buildroot required packages:**  
<https://buildroot.org/downloads/manual/manual.html#requirement>

```
cd axiom-evi-buildroot

# load evi buildroot default configution
make evi_ultrascale_defconfig

# customize busybox, linux or buildroot (optional)
make busybox-menuconfig
make linux-menuconfig
make menuconfig

# compile kernel and file system
make
```

NOTES:
```
If uboot fails with error like this:
        bin/sh: 1: dtc: not found
        make[2]: *** [arch/arm/dts/zynq-zc702.dtb] Error 127
You need to install the device tree compiler:
        eg. sudo apt-get install device-tree-compiler
```

### 2.3 AXIOM switch

Used to emulate the topology between multiple VMs

```
cd axiom-evi-nic/axiom_switch/
make
```

### 2.4 AXIOM NIC device driver

Device driver for the AXIOM Network Card

```
cd axiom-evi-nic/axiom_netdev_driver/

# compile and install in the buildroot target image
make install

# (after that, you must rebuild buildroot: cd ../axiom-buildroot; make)
```

### 2.5 AXIOM applications

Axiom user space application
(eg. axiom-init, axiom-whoami, axiom-send-small, axiom-recv-small, etc.)

```
cd axiom-evi-apps

# compile and install in the buildroot target image
make install

# (after that, you must rebuild buildroot: cd ../axiom-buildroot; make)
```




## 3. Run simulation

In the "scripts" folder there is a Makefile to run a simulator.
```
cd scripts
make VMS=<number of nodes> run log
```
Example:
```
# run 4 nodes with default axiom-switch (ring topology)
make VMS=4 run
# show the log file
make log
# shutdown the vms and the switch
make stop

# run 4 nodes and show log
make VMS=4 run log
# (CTRL-C) to shutdown the vms and the switch
```



## 4. Run application inside the nodes

During the 2.4 and 2.5 steps the AXIOM NIC device driver and the user-space
applications are compiled and installed in the nodes filesystem.

After the boot, all nodes autologin with root privileges.
Otherwise the credentials are:

 * user: root
 * password: *empty*

The AXIOM NIC device driver and the AXIOM ethernet tap (user space application) should start
on device startup.
You must choose a node and set it as master executing this command:
```
axiom-make-master
```
After that this node reconfigure itself as "master node", end the node discovery protocol and
configure an ethernet device for intra-node comunication (assigning an IP address).

#### run applications in the nodes:

 * axiom-info
    + print informations (node-id, interfaces, routing, etc.) about AXIOM NIC
```
        example:
        # print all information
        axiom-info
```
```
        # print nodeid
        axiom-info -n
```
```
        # print routing table
        axiom-info -r
```
 * axiom-whoami
     + print the node-id set after the discovery phase
```
        example:
        axiom-whoami
```
 * axiom-ping
    + estimate the round trip time (RTT) between two nodes
```
        example:
        # Estimate RTT with target node 2. Send a ping message every 0.5 seconds.
        axiom-ping -d 2 -i 0.5
```
```
        # Estimante RTT with target node 2. Send 10 ping message every 0.2 seconds.
        axiom-ping -d 2 -c 10 -i 0.2
```
 * axiom-netperf
    + estimate the throughput between two nodes
```
        example:
        # Estimate the throughput with targat node 3, sending 512 KBytes of data
        # using RAW messages.
        axiom-netperf -d 3 -l 512K -t raw
```
```
        # Estimate the throughput with targat node 3, sending 2 MBytes of data.
        # using LONG messages.
        axiom-netperf -d 3 -l 2M -t long
```
 * axiom-traceroute
    + print the hops needed to reach a specified target node
```
        example:
        # Print the hops needed to reach the node 1
        axiom-traceroute -d 1
```
 * axiom-[send|recv]
     + send/receive Axiom RAW/LONG data to/from a node
```
        examples:
        # receive RAW message
        axiom-recv -t raw
```
```
        # receive LONG message
        axiom-recv -t long
```
```
        # send RAW data to node (node_id=4) with a specified payload
        axiom-send -t raw -d 4   112 43 0 57 33
```
```
        # send RAW data to a neighbour directly connected on a local
        # interface (if_id=1) with a specified payload
        axiom-send -t raw -d 1 --neighbour   67 52 1 0 1 0
```
```
        # send LONG data to node (node_id=0) with a specified payload
        axiom-send -t long -d 0   112 43 0 57 33
```
 * axiom-run
    + spawn application on multiple nodes
```
        # run ls on remote node 1 (-n) and redirect (-r) the stdin/stdout/stderr
        axiom-run -n 1 -r ls
```
```
        # run ls on all nodes, redirect (-r) the stdin/stdout/stderr and print
        # the node id in each line (-i)
        axiom-run -r -i ls
```
 * axiom-rdma
     + use RDMA features (remote write, remote read) of Axiom NIC
```
        examples:
        # write 3 Kbytes in the RDMA zone of remote node reading from the local
        # RDMA zone filled with the bytes (1, 2, 3, 4) repeated many times to
        # fill the size specified (add -v to see the dump)
        axiom-run -P all -n 1,2 axiom-rdma -m w -s 3k 1 2 3 4
```
```
        # read 1 Kbytes in the RDMA zone of remote node reading from the remote
        # RDMA zone filled with the bytes (5, 0, 1) repeated many times to
        # fill the size specified (add -v to see the dump)
        axiom-run -P all -n 1,2 axiom-rdma -m r -s 1k 5 0 1
```
## 5. New system compilation

### 5.1 Prepare the system

Change the environent variables into $(AXIOM_HOME)/settings.sh

* PETALINUX

You must have installed PetaLinux; this must point to the installation directory
(default $HOME/petalinux)

* AXIOMBSP

You must have installed Seco BSP; this must point to the installation directory
(default $HOME/bsp/AXIOM-ZU9EG-2016.3)

* LINARO

You must have installed a linaro toolchains; this must point to the installation directory
(default $HOME/linaro)

Seco Ubunto image use
gcc-linaro-5.3.1-2016.05-x86_64_aarch64-linux-gnu.tar.xz
runtime-gcc-linaro-5.3.1-2016.05-aarch64-linux-gnu.tar.xz
sysroot-glibc-linaro-2.21-2016.05-aarch64-linux-gnu.tar.xz

the linaro installation directory must have the following layout:

linaro/
linaro/runtime-gcc-linaro-5.3.1-2016.05-aarch64-linux-gnu
linaro/sysroot-glibc-linaro-2.21-2016.05-aarch64-linux-gnu
linaro/gcc-linaro-5.3.1-2016.05-x86_64_aarch64-linux-gnu
linaro/host (link to gcc-linaro-5.3.1-2016.05-x86_64_aarch64-linux-gnu)
lunaro/sysroot (link to sysroot-glibc-linaro-2.21-2016.05-aarch64-linux-gnu)

After the installation you must create a symbolic link from usr to linaro/host directory:
cd ~/linaro/host
ln -s . usr

Then you must create a link from pkg-config to aarcha64-linux-pkg-config:
cd ~/linaro/host/usr/bin
ln -s /usr/bin/pkg-config aarcha64-linux-pkg-config

* ROOTFS

Yout must have extracted the Seco Ubuntu filesystem image; this must point to the installation directory
(default $HOME/zynq-rootfs)

Usually $HOME/zynq-rootfs is a link to $HOME/zynq-ubuntu-minimal_16.04-xenial_v1.2

* ROOTFSARCH

You must have a copy of the rootfs archive
(default $HOME/zynq-ubuntu-minimal_16.04-xenial_v1.2.tar.gz)

### 5.2 Compile the system

#### 5.2.1 Include settings

To compile the system you must include the settings.sh into ypur enviroment

bash$ source axiom-evi/settings.sh

#### 5.2.2 The enviroment

If you go into the scripts directory and do the command "make" a help will be show

bash:~$ cd ~/axiom-evi/scripts
bash:~/axiom-evi/scripts$ make

Makefile:33: *** Command line enviroment variables not defined!
You must define some variable on the command line (for example "make P=1 E=0 run"):
* KERN (required)
    br: use buildroot kernel
    seco: use seco bsp kernel (actually qemu does not work with this kernel)
* FS (optional)
    br: use buildroot root fs
    seco: use seco (ubuntu xenial) root fs (default)
* QEMU_SNAP (optional, used only during qemu run)
    0: the rootfs in on a static qcow2 (one for every machine)
    1: the rootfs in on a temp qcow2 snapshot (default)
* DISABLE_INSTR (optional, used during libraries compilation)
    0: compile EXTRAE and nic user library with instrumentation support (default)
    1: disable EXTRAE compilation and nic user library instrumentation
).  Stop.

#### 5.2.3 configure the system

bash:~/axiom-evi/scripts$ make KERN=XXX FS=XXX DISABLE_INSTR=XXX configure

where XXX is the values specified in the 5.2.2 section

#### 5.2.4 compile the system

bash:~/axiom-evi/scripts$ make KERN=XXX FS=XXX DISABLE_INSTR=XXX

where XXX is the values specified in the 5.2.2 section

### 5.3 Note for X86 compilation

* the user that compile the software must be into the sudoers file
  (but it is not necessary to use a "sudo make .....")

* the system shell should be bash
  so there must be a link from /bin/sh to bash

* the linux-tools-generic and linux-headers-generic packages must be installed
